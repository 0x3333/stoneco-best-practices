![alt text](static/mongodb-logo.png "MongoDB")

# MongoDB Best Practices

## Index

* [Introduction](#introduction)
* [Best Practices](#best-practices)
* [References](#references)

## Introduction

MongoDB is an open-source document database that provides high performance, high availability, and automatic scaling.

A record in MongoDB is a document, which is a data structure composed of field and value pairs. The values of fields may include other documents, arrays, and arrays of documents.

The advantages of using documents are:
* Documents (i.e. objects) correspond to native data types in many programming languages.
* Embedded documents and arrays reduce need for expensive joins.
* Dynamic schema supports fluent polymorphism.

Documents are stored in collections. A collection in MongoDb is comparable to a table in a relational database, with the difference that the documents in a collection does not need to have the same scema.

Every document has the field "_id". If this field is not provided by the user, it will be automatically generated.

## Best Practices

### Create indexes

One of the most important things in any data base system are indexes. They are very important to improve query performance, so indexes should be created to support queries.
On the other hand, indexes spend space and, more important, maintaining much indexes makes lower inserting performance, so don’t create indexes that queries do not use.

MongoDb always create a default unique _id index for every collection. This index cannot be dropped.

When you create indexes you can define order ascending or descending. Next index types are supported:
* __Single field__: an index over a single field of the documents.
* __Compound index__: an index over multiple fields. The order can be defined for each of the fields.
* __Multikey index__: a special type of index generated by MongoDb when  you create an index where at least one field is an array.
* __Geospatial index__: index for geospatial coordinate data. MongoDB support 2 types of geospatial index:
..* _2d index_ uses planar geometry. You can find more information about this type of index [here](https://docs.mongodb.com/manual/core/2d/)
..* _2dsphere index_ uses spherical geometry. You can find more information about this type of index [here](https://docs.mongodb.com/manual/core/2dsphere/)
* __Text index__: index to support text queries over string content. It does not store language-specific stop words like “the”, “a”, “or”, etc.
* __Hashed index__: indexes the hash of the value of a field instead the value itself. Due to their nature, these indexes only support equality matches and cannot support range-based queries.

Moreover, indexes can have special properties:
* __Unique__: If an index is defined as unique, duplicate values for the indexed field will be rejected.
* __Sparse__: If an index is defined as sparse, it will skip documents that do not have the indexed field .
* __TTL__: It will remove documents from a collection after a certain amount of time.

When you create indexes, you must keep in mind the next limitations:
* A single collection can have no more than 64 indexes.
* Fully qualified index names, which includes the namespace and the dot separators (i.e. __<database name>.<collection name>.$<index name>__), cannot be longer than 128 characters.
* There can be no more than 31 fields in a compound index.
* Queries cannot use both text and Geospatial Indexes.
* Fields with 2dsphere Indexes can only hold Geometries.
* For a compound multikey index, each indexed document can have at most one indexed field whose value is an array. As such, you cannot create a compound multikey index if more than one to-be-indexed field of a document is an array. Or, if a compound multikey index already exists, you cannot insert a document that would violate this restriction. So be careful with it.
* The total size of an index entry must be less than 1024 bytes.

### Always use replica sets

A replica set in MongoDB is a group of mongod processes that maintain the same data set. 
Replication provides high availability of your data if node fails in your cluster. Replica Set provides automatic failover mechanism. If your primary node fails, a secondary node will be elected as primary and your cluster will remain functional. 
In some cases, replication can provide increased read capacity as clients can send read operations to different servers.
In a replica set there always is a primary node that receives write operations and some secondaries that replicate the data thorught an oplog, applying operations asynchronously.
You should replicate with at least 3 nodes in any MongoDB deployment.

If the primary node goes down, a secondary must be elected to be the new primary. This is done with an election where each node emites a vote. Therefore, it is highly recommended to have an odd number of nodes.
If you have an even number of nodes in your replica set, and you do not want to increase your hardware for break down elections, use an arbiter node. An arbiter is a node of the replica set that do not maintain data but takes part in the election. An arbiter do not maintain data, so it is not elegible to be the primary, thus you do not need a great hardware on it.
Anyway, remember that, although a replica set can have up to 50 members (12 members before version 3.0), there can be no more than seven voting members in a replica set.

In any cases you may want to have secondary nodes in a replica set that are not be able to convert to primary members. In these cases yo must set priority 0 for those nodes.

Aditionally, you could want to have some special node for dedicated task. In this case you should also mark the node as hidden. Doing it, you maintain that node invisible to client applications. Furthermore, in a sharded cluster, balancers do not interact with hidden members. So a hidden member will not receive read operations.
This is an example of hidding the member at the index 0 in the members array of a replica set:
```javascript
cfg = rs.conf()
cfg.members[0].priority = 0
cfg.members[0].hidden = true
rs.reconfig(cfg)
```

Often, human errors take place. To help to recover these errors when they are detected in a reasonable time, you can set a delayed member in your replica set.
A delayed member is a hidden member that apply de oplog operations with a delay. Therefore you could recover from it the data that you had in realtime nodes the "delayed" time ago.
So, you should apply big enought delay to your expected recover or maintenance duration. On the other hand, the delay must not be big enought to exceed the capacity of the oplog.
Nevertheless, delayed members are not recommended in sharded clusters because of the possible chunk migrations during the delay. 

### Your working set should fit in memory

The working set is the set of data and indexes accessed during normal operations. Proper capacity planning is important for a highly performant application
Being able to keep the working set in memory is an important factor in overall cluster performance.
MongoDB works best when the data set can reside in memory. Nothing performs better than a MongoDB instance that does not require disk I/O. 
Whenever possible select a platform that has more available RAM than your working data set size.

If you notice the number of page faults increasing, there is a very high probability that your working set is larger than your available RAM.
When this happens, you should increase your instance RAM size. If you can do it, consider using sharding to increase the amount of available RAM in a cluster.

### Scale up if your metrics show heavy use

If your instance shows a load over 60% - 65%, you should consider scaling up. Your load should be consistently below this threshold during normal operations. This also impacts recovery and vertical scaling scenarios.
At the moment you identify that you wanted to scale, you should consider sharding. By sharding, MongoDB distributes the data across sharded cluster.

### When to shard

In addition to mentioned previously, you should consider sharding if you anticipate a large data set.
Sharding may also help write performance so it is also possible that you may elect to shard even if your data set is small but requires a high amount of updates or inserts.

However, sharding may not be the solution to a bad performance. If you have worse performance than expected, you should reconsider your schema and indexes.

When you decide to shard, a very important decision you maust take is the choice of the shard key, since the distribution of the data will depend on it, and affects the overall efficiency and performance of operations within the sharded cluster. 
Furthermore shard key is inmutable, so once you create a shard key you will not be able to change it.

To shard a collection use the method `sh.shardCollection()` specifying the target collection and the shard key.
All sharded collections must have an index that supports the shard key; i.e. the index can be an index on the shard key or a compound index where the shard key is a prefix of the index. So if you want to shard a non empty collection, you previously must create the index. However, if the collection is empty, the index will be created if it does not exist.

The choice of a shard key will depend on your data and their nature. Based on it, you should keep in mind some important criteria.

The first (and obvious) consideration is that every document in the collection must contain the fields that be part of the shard key, otherwise it would be impossible to know where the document should be localized.

For the choice of the shard key, you will have to consider the importance of distribuiton of data, write performance and read performance, and which is more important to you. Depending on it, to choose a shard key as good as possible try to:
* Create a shard key easily divisible. For it, consider fields (or combination of fields) that can take a large number of distinct values, and that are not expected to have the same value in many different documents. This will provide distribution of data.
* Create a shard key with high degree of randomness. In this way write operations will be distributed among the cluster, preventing that a single shard becomes a bottleneck. This will provide write performance.
..* Do not create monotonically changing shard key. A shard key on a value that always increases or that always decreases is more likely to distribute inserts to a single shard within the cluster, becoming it a bottleneck.
..* If the nature of your data makes dificult to choose an enought random shard key or you are considering to select a monotically changing field because of other selecting criteria, you may consider to use a hashed index for the shard key. In this case you can only use a single field. 
* Create a shard key that targets a single shard. Thus, balancer can easily redirect queries to specific shard. This will provide read performance.
* Create a shard key based on a compound index. Selecting a group of fields as the shard key instead of a single field, will facilitate you to get a more ideal shard key.
 
In addition, whe you are going to create the shard key, you must keep in mind the next limitations:
* If the shard key index is not a hasehd index, then the index, or the prefix part of the index corresponding to shard key must have ascending order.
* A shard key index cannot be an index that specifies a multikey index, a text index or a geospatial index on the shard key fields.
* As mentioned, shard key is inmutable.
* Shard key value in a document is inmutable, this is, you cannot update the values of the shard key fields.
* A shard key cannot exceed 512 bytes.


### Keep each mongo instance on its own machine

Mongo instances always try to use as resources as it can. So you should not run more than one instance on the same machine.
If you run more than one mongo instance in a single machine, all of that instances will fight for the same resources.
...

### Turn journaling on by default

MongoDB supports write-ahead journaling of operations to facilitate crash recovery and node durability.
Journaling basically holds write-ahead redo logs, in the event of crash, the journal files will be used for recovery and this enables data consistency and durability.
Journal process differs depending on storage engine. Journaling is recommended storage engines that make use of disk, like MMAPv1 or the newer WiredTiger.
Nevertheless, for the new In-Memory Storage Engine of MongoDB Enterprise version 3.2.6, there is no separate journal, because its data is kept in memory.

### Hardware
32bit/64bit
disk
CPU
...

### Testing
...

### Monitoring
...

### Keep current with versions

Keep your version of MongoDB current. Each release has significant performance enhancements, improvements and fixes.

## References

* [MongoDB documentation](https://docs.mongodb.com/manual/)
