![alt text](static/mongodb-logo.png "MongoDB")

# MongoDB Best Practices

## Index

* [Introduction](#introduction)
* [Best Practices](#best-practices)
* [Storage engine] (#storage-engine)
* [References](#references)

## Introduction

MongoDB is an open-source document database that provides high performance, high availability, and automatic scaling.

A record in MongoDB is a document, which is a data structure composed of field and value pairs. The values of fields may include other documents, arrays, and arrays of documents.

The advantages of using documents are:
* Documents (i.e. objects) correspond to native data types in many programming languages.
* Embedded documents and arrays reduce need for expensive joins.
* Dynamic schema supports fluent polymorphism.

Documents are stored in collections. A collection in MongoDb is comparable to a table in a relational database, with the difference that the documents in a collection does not need to have the same scema.

Every document has the field "_id". If this field is not provided by the user, it will be automatically generated.

## Best Practices

### Create indexes

One of the most important things in any data base system are indexes. They are very important to improve query performance, so indexes should be created to support queries.
On the other hand, indexes spend space and, more important, maintaining much indexes makes lower inserting performance, so don’t create indexes that queries do not use.

MongoDb always create a default unique _id index for every collection. This index cannot be dropped.

When you create indexes you can define order ascending or descending. Next index types are supported:
* __Single field__: an index over a single field of the documents.
* __Compound index__: an index over multiple fields. The order can be defined for each of the fields.
* __Multikey index__: a special type of index generated by MongoDb when you create an index where at least one field is an array.
* __Geospatial index__: index for geospatial coordinate data. MongoDB support 2 types of geospatial index:
  * _2d index_ uses planar geometry. You can find more information about this type of index [here](https://docs.mongodb.com/manual/core/2d/)
  * _2dsphere index_ uses spherical geometry. You can find more information about this type of index [here](https://docs.mongodb.com/manual/core/2dsphere/)
* __Text index__: index to support text queries over string content. It does not store language-specific stop words like “the”, “a”, “or”, etc.
* __Hashed index__: indexes the hash of the value of a field instead the value itself. Due to their nature, these indexes only support equality matches and cannot support range-based queries.

Moreover, indexes can have special properties:
* __Unique__: If an index is defined as unique, duplicate values for the indexed field will be rejected.
* __Sparse__: If an index is defined as sparse, it will skip documents that do not have the indexed field .
* __TTL__: It will remove documents from a collection after a certain amount of time.

When you create indexes, you must keep in mind the next limitations:
* A single collection can have no more than 64 indexes.
* Fully qualified index names, which includes the namespace and the dot separators (i.e. __<database name>.<collection name>.$<index name>__), cannot be longer than 128 characters.
* There can be no more than 31 fields in a compound index.
* Queries cannot use both text and Geospatial Indexes.
* Fields with 2dsphere Indexes can only hold Geometries.
* For a compound multikey index, each indexed document can have at most one indexed field whose value is an array. As such, you cannot create a compound multikey index if more than one to-be-indexed field of a document is an array. Or, if a compound multikey index already exists, you cannot insert a document that would violate this restriction. So be careful with it.
* The total size of an index entry must be less than 1024 bytes.

### Always use replica sets

A replica set in MongoDB is a group of mongod processes that maintain the same data set. 
Replication provides high availability of your data if node fails in your cluster. Replica Set provides automatic failover mechanism. If your primary node fails, a secondary node will be elected as primary and your cluster will remain functional.  
In some cases, replication can provide increased read capacity as clients can send read operations to different servers.
In a replica set there always is a primary node that receives write operations and some secondaries that replicate the data thorught an oplog, applying operations asynchronously.
You should replicate with at least 3 nodes in any MongoDB deployment.

If the primary node goes down, a secondary must be elected to be the new primary. This is done with an election where each node emites a vote. Therefore, it is highly recommended to have an odd number of nodes.
If you have an even number of nodes in your replica set, and you do not want to increase your hardware for break down elections, use an arbiter node. An arbiter is a node of the replica set that do not maintain data but takes part in the election. An arbiter do not maintain data, so it is not elegible to be the primary, thus you do not need a great hardware on it.
Anyway, remember that, although a replica set can have up to 50 members (12 members before version 3.0), there can be no more than seven voting members in a replica set.

In any cases you may want to have secondary nodes in a replica set that are not be able to convert to primary members. In these cases yo must set priority 0 for those nodes.

Aditionally, you could want to have some special node for dedicated task. In this case you should also mark the node as hidden. Doing it, you maintain that node invisible to client applications. Furthermore, in a sharded cluster, balancers do not interact with hidden members. So a hidden member will not receive read operations.
This is an example of hidding the member at the index 0 in the members array of a replica set:
```javascript
cfg = rs.conf()
cfg.members[0].priority = 0
cfg.members[0].hidden = true
rs.reconfig(cfg)
```

Often, human errors take place. To help to recover these errors when they are detected in a reasonable time, you can set a delayed member in your replica set.
A delayed member is a hidden member that apply de oplog operations with a delay. Therefore you could recover from it the data that you had in realtime nodes the "delayed" time ago.
So, you should apply big enought delay to your expected recover or maintenance duration. On the other hand, the delay must not be big enought to exceed the capacity of the oplog.
Nevertheless, delayed members are not recommended in sharded clusters because of the possible chunk migrations during the delay. 

### Your working set should fit in memory

The working set is the set of data and indexes accessed during normal operations. Proper capacity planning is important for a highly performant application
Being able to keep the working set in memory is an important factor in overall cluster performance.
MongoDB works best when the data set can reside in memory. Nothing performs better than a MongoDB instance that does not require disk I/O. 
Whenever possible select a platform that has more available RAM than your working data set size.

If you notice the number of page faults increasing, there is a very high probability that your working set is larger than your available RAM.
When this happens, you should increase your instance RAM size. If you can do it, consider using sharding to increase the amount of available RAM in a cluster.

### Scale up if your metrics show heavy use

If your instance shows a load over 60% - 65%, you should consider scaling up. Your load should be consistently below this threshold during normal operations. This also impacts recovery and vertical scaling scenarios.
At the moment you identify that you wanted to scale, you should consider sharding. By sharding, MongoDB distributes the data across sharded cluster.

### When to shard

In addition to mentioned previously, you should consider sharding if you anticipate a large data set.
Sharding may also help write performance so it is also possible that you may elect to shard even if your data set is small but requires a high amount of updates or inserts.

However, sharding may not be the solution to a bad performance. If you have worse performance than expected, you should reconsider your schema and indexes.

When you decide to shard, a very important decision you maust take is the choice of the shard key, since the distribution of the data will depend on it, and affects the overall efficiency and performance of operations within the sharded cluster. 
Furthermore shard key is inmutable, so once you create a shard key you will not be able to change it.

To shard a collection use the method `sh.shardCollection()` specifying the target collection and the shard key.
All sharded collections must have an index that supports the shard key; i.e. the index can be an index on the shard key or a compound index where the shard key is a prefix of the index. So if you want to shard a non empty collection, you previously must create the index. However, if the collection is empty, the index will be created if it does not exist.

The choice of a shard key will depend on your data and their nature. Based on it, you should keep in mind some important criteria.

The first (and obvious) consideration is that every document in the collection must contain the fields that be part of the shard key, otherwise it would be impossible to know where the document should be localized.

For the choice of the shard key, you will have to consider the importance of distribuiton of data, write performance and read performance, and which is more important to you. Depending on it, to choose a shard key as good as possible try to:
* Create a shard key easily divisible. For it, consider fields (or combination of fields) that can take a large number of distinct values, and that are not expected to have the same value in many different documents. This will provide distribution of data.
* Create a shard key with high degree of randomness. In this way write operations will be distributed among the cluster, preventing that a single shard becomes a bottleneck. This will provide write performance.
  * Do not create monotonically changing shard key. A shard key on a value that always increases or that always decreases is more likely to distribute inserts to a single shard within the cluster, becoming it a bottleneck.
  * If the nature of your data makes dificult to choose an enought random shard key or you are considering to select a monotically changing field because of other selecting criteria, you may consider to use a hashed index for the shard key. In this case you can only use a single field. 
* Create a shard key that targets a single shard. Thus, balancer can easily redirect queries to specific shard. This will provide read performance.
* Create a shard key based on a compound index. Selecting a group of fields as the shard key instead of a single field, will facilitate you to get a more ideal shard key.
 
In addition, whe you are going to create the shard key, you must keep in mind the next limitations:
* If the shard key index is not a hasehd index, then the index, or the prefix part of the index corresponding to shard key must have ascending order.
* A shard key index cannot be an index that specifies a multikey index, a text index or a geospatial index on the shard key fields.
* As mentioned, shard key is inmutable.
* Shard key value in a document is inmutable, this is, you cannot update the values of the shard key fields.
* A shard key cannot exceed 512 bytes.


### Turn journaling on by default

MongoDB supports write-ahead journaling of operations to facilitate crash recovery and node durability.
Journaling basically holds write-ahead redo logs, in the event of crash, the journal files will be used for recovery and this enables data consistency and durability.
Journal process differs depending on storage engine. Journaling is recommended storage engines that make use of disk, like MMAPv1 or the newer WiredTiger.
Nevertheless, for the new In-Memory Storage Engine of MongoDB Enterprise version 3.2.6, there is no separate journal, because its data is kept in memory.

### Hardware

#### Keep each mongo instance on its own machine

Mongo instances always try to use as resources as it can. So you should not run more than one instance on the same machine.
If you run more than one mongo instance in a single machine, all of that instances will contend for the same resources.

#### Don't run MongoDB on 32-bit systems

MongoDB has a 2GB data limit on 32-bit system and 32-bit systems has memory limitations too, so you should have mongo running on a 64-bit processor and not on 32-bit.
Furthermore, since version 3.0 MongoDB has not commercial support for 32bit platforms; and starting in MongoDB 3.2, 32-bit binaries are deprecated and will be unavailable in future releases.

#### Use Solid State Disks (SSD)

MongoDB has good results and a good price-performance ratio with SATA SSD.

Use SSD if available and economical. Spinning disks can be performant, but the capacity of SSD drives for random I/O operations works well with the update model of MMAPv1.

Commodity (SATA) spinning drives are often a good option, as the random I/O performance increase with more expensive spinning drives is not that dramatic (only on the order of 2x). But using SSD drives may be more effective in increasing I/O throughput.

#### Prefer local disks

Local disks should be used if possible as network storage can cause high latency and poor performance for your deployment.

With the MMAPv1 storage engine, the Network File System protocol (NFS) is not recommended as you may see performance problems when both the data files and the journal files are hosted on NFS. You may experience better performance if you place the journal on local or iscsi volumes.

With the WiredTiger storage engine, WiredTiger objects may be stored on remote file systems if the remote file system conforms to ISO/IEC 9945-1:1996 (POSIX.1). Because remote file systems are often slower than local file systems, using a remote file system for storage may degrade performance.

If you decide to use NFS, add the following NFS options to your /etc/fstab file: bg, nolock, and noatime.

#### Use RAID-10

With solid state drives seek-time is significantly reduced. SSDs also provide more gradual performance degradation if the working set no longer fits in memory.

If using disk arrays, the recommend is to use RAID-10, as RAID-5 and RAID-6 do not provide sufficient performance. RAID-0 offers good write performance but limited read performance and insufficient fault tolerance.

#### Separate Components onto Different Storage Devices

For improved performance, consider separating your database’s data, journal, and logs onto different storage devices, based on your application’s access and write pattern. Mount the components as separate filesystems and use symbolic links to map each component’s path to the device storing it.

For the WiredTiger storage engine, you can also store the indexes on a different storage device.

#### Swap

Assign swap space for your systems. Allocating swap space can avoid issues with memory contention and can prevent the OOM Killer on Linux systems from killing mongod.

For the MMAPv1 storage engine, the method mongod uses to map files to memory ensures that the operating system will never store MongoDB data in swap space. On Windows systems, using MMAPv1 requires extra swap space due to commitment limits.

For the WiredTiger storage engine, given sufficient memory pressure, WiredTiger may store data in swap space.


#### MongoDB and NUMA Hardware

Running MongoDB on a system with Non-Uniform Access Memory (NUMA) can cause a number of operational problems, including slow performance for periods of time and high system process usage.

When running MongoDB servers and clients on NUMA hardware, you should configure a memory interleave policy so that the host behaves in a non-NUMA fashion. MongoDB checks NUMA settings on start up when deployed on Linux (since version 2.0) and Windows (since version 2.6) machines. If the NUMA configuration may degrade performance, MongoDB prints a warning.

#### CPU

MongoDB will deliver better performance on faster CPUs. When running a MongoDB instance with the majority of the data being in memory, clock speed can have a major impact on overall performance.

For the MMAPv1 storage engine, the core speed is clearly more important than the number of cores. Increasing the number of cores will not improve performance significantly.

However the WiredTiger storage engine is multithreaded and can take advantage of additional CPU cores. 

### Monitoring and testing

An important thing to do once you have determined yor deployment stratedy is to test it with a data set similar to your production data.

Test within the context of your application and against traffic patterns that are representative of your production system. A test environment that does not resemble your production traffic will block you from discovering performance bottlenecks and architectural design flaws.

MongoDB provides some tools that will help you to test and monitor your deployment.

It provides some products like __[Mongo MMS](https://www.mongodb.com/post/10764757533/announcing-mongodb-monitoring-service-mms)__, now called __[MongoDB Cloud Manager](https://www.mongodb.com/cloud/cloud-manager)__, a SaaS based tool that monitors your MongoDB cluster and makes it easy for you to see what's going on in a production deployment; or __[MongoDB Ops Manager](https://www.mongodb.com/products/ops-manager)__, available in MongoDB Enterprise Advanced.

Some useful monitoring utilities in MongoDB are:
* __mongostat__ captures and returns the counts of database operations by type (e.g. insert, query, update, delete, etc.). These counts report on the load distribution on the server. More info about this utility [here](https://docs.mongodb.com/manual/reference/program/mongostat/)
* __mongotop__ tracks and reports the current read and write activity of a MongoDB instance, and reports these statistics on a per collection basis. More info about this utility [here](https://docs.mongodb.com/manual/reference/program/mongotop/)
* __HTTP console__ (Deprecated since version 3.2) is a HTTP interface for MongoDB that exposes diagnostic and monitoring information in a simple web page. 

Also provides some useful commands:
* __serverStatus__ returns a general overview of the status of the database, detailing disk usage, memory use, connection, journaling, and index access. The command returns quickly and does not impact MongoDB performance. More info about this command [here](https://docs.mongodb.com/manual/reference/command/serverStatus/)
* __dbStats__ returns a document that addresses storage use and data volumes. The dbStats reflect the amount of storage used, the quantity of data contained in the database, and object, collection, and index counters. More info about this command [here](https://docs.mongodb.com/manual/reference/command/dbStats/)
* __collStats__ provides statistics that resemble dbStats on the collection level, including a count of the objects in the collection, the size of the collection, the amount of disk space used by the collection, and information about its indexes. More info about this command [here](https://docs.mongodb.com/manual/reference/command/collStats/)
* __replSetGetStatus__ returns an overview of your replica set’s status. The replSetGetStatus document details the state and configuration of the replica set and statistics about its members. More info about this command [here](https://docs.mongodb.com/manual/reference/command/replSetGetStatus/)

There are also some third party tools:
* __Self Hosted__ (you must install, configure and maintain on your own servers). Most are open source. 
  * __[mongodb-ganglia](https://github.com/quiiver/mongodb-ganglia)__: Python script to report operations per second, memory usage, btree statistics, master/slave status and current connections.
  * __[gmond_python_modules](https://github.com/ganglia/gmond_python_modules)__: Parses output from the serverStatus and replSetGetStatus commands.
  * __[motop](https://github.com/tart/motop)__: Realtime monitoring tool for MongoDB servers. Shows current operations ordered by durations every second.
  * __[mtop](https://github.com/beaufour/mtop)__: A top like tool.
  * __[mongo-munin](https://github.com/erh/mongo-munin)__: Retrieves server statistics.
  * __[mongomom](https://github.com/pcdummy/mongomon)__: Retrieves collection statistics (sizes, index sizes, and each (configured) collection count for one DB).
  * __[nagios-plugin-mongodb](https://github.com/mzupan/nagios-plugin-mongodb)__: A simple Nagios check script, written in Python.
  * __[spm-agent-mongodb](https://hub.docker.com/r/sematext/spm-agent-mongodb/)__: Monitoring, Anomaly Detection and Alerting SPM monitors all key MongoDB metrics together with infrastructure incl. Docker and other application metrics e.g. Node.js, Java, NGINX, Apache, HAProxy or Elasticsearch. SPM is available On Premises and in the Cloud (SaaS) and provides correlation of metrics and logs.
* __Hosted (SaaS) Monitoring Tools__. These are monitoring tools provided as a hosted service, usually through a paid subscription. More info [here](https://docs.mongodb.com/manual/administration/monitoring/#self-hosted-monitoring-tools)

MongoDB also can provide database metrics via SNMP, available for [Linux](https://docs.mongodb.com/manual/tutorial/monitor-with-snmp/) and [Windows](https://docs.mongodb.com/manual/tutorial/monitor-with-snmp-on-windows/), but only in MongoDB Enterprise Advanced.

### Keep current with versions

Keep your version of MongoDB current. Each release has significant performance enhancements, improvements and fixes.


## Storage Engine

This component is responsible of managing how data is stored, both in memory and on disk. There are many storage engines that you can choose to adapt to your application with the purpose of obtaining an improvement of performance.
In MongoDB 3.2 the default storage engine is WiredTiger. It provides a document-level concurrency model, checkpointing, compression and other features. On lower versions the default storage engine is MMAPv1, which works well with high volumes of reads and writes, as well as in place updates.


### WiredTiger Storage Engine

WiredTiger designed to maximize the performance of multi-core hardware and minimize the disk access thanks to the use of a compact file format and data compression. It provides a set of utilities for storage, which are detailed below:

####Document level concurrency

WiredTiger uses document-level concurrency control for write operations. As a result, multiple clients can modify different documents of a collection at the same time, Which substantially increases the performance of MongoDB.

For most read and write operations, WiredTiger uses optimistic concurrency control. WiredTiger uses only intent locks at the global, database and collection levels. When the storage engine detects conflicts between two operations, one will incur a write conflict causing MongoDB to transparently retry that operation.

####Snapshots and Checkpoints

WT provides a MultiVersion Concurrency Control (MVCC) that uses a point-in-time snapshot of the data to present a consistent view of the in-memory data.

When a write operation arrives, WT writes all data in a snapshot to disk. This data now acts as a checkpoint that ensures the data files are consistent up to and including the last checkpoint. This makes possible that a checkpoint can be used for recovering purposes. MongoDB configures WT to create checkpoints at intervals of 60 seconds or 2 gigabytes of journal data.

The new checkpoint becomes accessible and permanent when WiredTiger’s metadata table is atomically updated to reference the new checkpoint. Once the new checkpoint is accessible, WT frees pages from the old checkpoints.

You can recover from the last checkpoint using WT, but all the data written since the last checkpoint will be lost. To can recovery this data you must to use journaling.

####Journal

WT uses a write-ahead transaction log in combination with checkpoints to ensure data durability. WT uses journaling to persist all write operations between checkpoints. The journal is compressed using snappy library.
Journaling is important for standalone instances to avoid losing data when Mongo falls down between checkpoints. It isn’t as critical for replica set members because the replication process provides sufficient durability for our data.

####Compression

WT uses block compression with the snappy library for all collections and a prefix compression for all indexes. We can use for collections the zlib compression library too. Compression settings are also configurable on a per-collection and per-index basis during collection and index creation.

####Memory use

With WT, Mongo uses the internal cache of WT and the filesystem cache. The WT cache uses a 60% of RAM minus 1 GB or 1GB, whichever is larger.


### MMAPv1 Storage Engine

MMAPv1 is MongoDB’s original storage engine based on memory mapped files. It excels at workloads with high volume inserts, reads, and in-place updates.

####Journal

MongoDB, by default, records all modifications to an on-disk journal. MongoDB writes more frequently to the journal than it writes the data files (MongoDB writes to the data files on disk every 60 seconds and writes to the journal files roughly every 100 milliseconds).
The journal allows MongoDB to successfully recover data from data files after a mongod instance exits without flushing all changes.

####Record Storage Characteristics

All records are contiguously located on disk, and when a document becomes larger than the allocated record, MongoDB must allocate a new record.

New allocations require MongoDB to move a document and update all indexes that refer to the document, which takes more time than in-place updates and leads to storage fragmentation. To avoid this situation we can use two different record allocation strategies:

* __Power of 2 sized allocations__: Because documents in MongoDB may grow after insertion and all records are contiguous on disk, the padding can reduce the need to relocate documents on disk following updates. Relocations are less efficient than in-place updates and can lead to storage fragmentation. As a result, all padding strategies trade additional space for increased efficiency and decreased fragmentation.

   With the power of 2 sizes allocation strategy, each record has a size in bytes that is a power of 2 (e.g. 32, 64, 128, 256, 512 ... 2    MB). For documents larger than 2 MB, the allocation is rounded up to the nearest multiple of 2 MB.  
   This allows the document to grow without having to reallocate it and to reduce fragmentation, because a new document can reuse freed    records.

   This strategy works more efficient for insert/update/delete workloads.

* __No padding allocation__: This strategy can be used for collections whose workloads do not change the document sizes, such as workloads that consist of insert-only operations or update operations that do not increase document size.

####Memory use

MMAPv1 uses all free memory on the machine as its cache. System resource monitors show that MongoDB uses a lot of memory, but its usage is dynamic. If another process suddenly needs half the server’s RAM, MongoDB will yield cached memory to the other process.
Technically, the operating system’s virtual memory subsystem manages MongoDB’s memory. This means that MongoDB will use as much free memory as it can, swapping to disk as needed. Deployments with enough memory to fit the application’s working data set in RAM will achieve the best performance.


### In-Memory Storage Engine

The in-memory storage engine is part of general availability (GA) in the 64-bit builds. This kind of memory doesn't persist the data on disk, including configuration data, indexes, user credentials, etc.

By avoiding disk I/O, the in-memory storage engine allows for more predictable latency of database operations.

####Concurrency

The in-memory storage engine uses document-level concurrency control for write operations. As a result, multiple clients can modify different documents of a collection at the same time.

####Memory use

By default, the in-memory storage engine uses 50% of physical RAM minus 1 GB.

####Durability

The in-memory storage engine is non-persistent and does not write data to a persistent storage. That is non-persisted data includes application data and system data, such as users, permissions, indexes, replica set configuration, sharded cluster configuration, etc.
As such, the concept of journal or waiting for data to become durable does not apply to the in-memory storage engine.

Write operations that specify a write concern journaled are acknowledged immediately. When a mongod instance shuts down, either as result of the shutdown command or due to a system error, recovery of in-memory data is impossible.


## References

* [MongoDB documentation](https://docs.mongodb.com/manual/)
